---
# Source: hkube/charts/jaeger/templates/agent-sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: jaeger-agent
  labels:
    helm.sh/chart: jaeger-0.27.2004
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: hkube
    app.kubernetes.io/version: "1.17.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: agent
---
# Source: hkube/charts/jaeger/templates/cassandra-schema-sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: jaeger-cassandra-schema
  labels:
    helm.sh/chart: jaeger-0.27.2004
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: hkube
    app.kubernetes.io/version: "1.17.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: cassandra-schema
---
# Source: hkube/charts/jaeger/templates/collector-sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: jaeger-collector
  labels:
    helm.sh/chart: jaeger-0.27.2004
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: hkube
    app.kubernetes.io/version: "1.17.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: collector
---
# Source: hkube/charts/jaeger/templates/query-sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: jaeger-query
  labels:
    helm.sh/chart: jaeger-0.27.2004
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: hkube
    app.kubernetes.io/version: "1.17.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: query
---
# Source: hkube/charts/minio/templates/post-install-prometheus-metrics-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hkube-minio-update-prometheus-secret
  labels:
    app: minio-update-prometheus-secret
    chart: minio-5.0.23
    release: hkube
    heritage: Helm
---
# Source: hkube/charts/minio/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: "hkube-minio"
  namespace: "default"
---
# Source: hkube/charts/redis-ha/templates/redis-ha-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hkube-redis-ha
  labels:
    heritage: Helm
    release: hkube
    chart: redis-ha-3.6.3
    app: hkube-redis-ha
---
# Source: hkube/templates/algorithm-builder-role.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: algorithm-builder-serviceaccount
  labels:
    group: hkube
    app: algorithm-builder
    core: "true"
---
# Source: hkube/templates/algorithm-operator-role.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: algorithm-operator-serviceaccount
  labels:
    group: hkube
    app: algorithm-operator
    core: "true"
---
# Source: hkube/templates/cleanup-role.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cleanup-serviceaccount
  labels:
    group: hkube
    app: cleanup
    core: "true"
---
# Source: hkube/templates/job-cleaner-role.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: default
  name: clean-old-jobs
---
# Source: hkube/templates/monitor-server-role.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: monitor-server-serviceaccount
  labels:
    group: hkube
    app: monitor-server
    core: "true"
---
# Source: hkube/templates/resource-manager-role.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: resource-manager-serviceaccount
  labels:
    group: hkube
    app: resource-manager
    core: "true"
---
# Source: hkube/templates/task-executor-role.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: task-executor-serviceaccount
  labels:
    group: hkube
    app: task-executor
    core: "true"
---
# Source: hkube/templates/task-executor-role.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: worker-serviceaccount
  labels:
    group: hkube
    app: task-executor
    core: "true"
---
# Source: hkube/charts/jaeger/templates/cassandra-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: jaeger-cassandra
  labels:
    helm.sh/chart: jaeger-0.27.2004
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: hkube
    app.kubernetes.io/version: "1.17.1"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  password: "cGFzc3dvcmQ="
---
# Source: hkube/charts/minio/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: hkube-minio
  labels:
    app: minio
    chart: minio-5.0.23
    release: hkube
    heritage: Helm
type: Opaque
data:
  accesskey: "aGt1YmVtaW5pb2tleQ=="
  secretkey: "aGt1YmVtaW5pb3NlY3JldA=="
---
# Source: hkube/templates/docker-credentials-secret.yaml
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: docker-credentials-secret
  namespace: default
  labels:
    group: hkube
    core: "true"
data:
  docker_pull_registry: ""
  docker_pull_namespace: "aGt1YmU="
  docker_pull_username: ""
  docker_pull_password: ""
  docker_pull_insecure: "ZmFsc2U="
  docker_pull_skip_tls_verify: "ZmFsc2U="
  
  docker_push_registry: ""
  
  docker_push_namespace: ""
  
  docker_push_username: ""
  docker_push_password: ""
  docker_push_insecure: "ZmFsc2U="
  docker_push_skip_tls_verify: "ZmFsc2U="


  npm_registry: ""
  npm_token: ""
  pip_registry: ""
  pip_token: ""

  # pull-registry: ""
  # pull-namespace: "aGt1YmU="
  # pull-username: ""
  # pull-password: ""
  
  # push-registry: ""
  # 
  # push-namespace: ""
  # 
  # push-username: ""
  # push-password: ""
---
# Source: hkube/templates/s3-secret.yaml
apiVersion: v1
data:
  awsEndpointUrl: "aHR0cDovL2hrdWJlLW1pbmlvOjkwMDA="
  awsKey: "aGt1YmVtaW5pb2tleQ=="
  awsSecret: "aGt1YmVtaW5pb3NlY3JldA=="
kind: Secret
metadata:
  name: s3-secret
  namespace: default
type: Opaque
---
# Source: hkube/charts/minio/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hkube-minio
  labels:
    app: minio
    chart: minio-5.0.23
    release: hkube
    heritage: Helm
data:
  initialize: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/etc/minio/mc/"
    MC="/usr/bin/mc --config-dir ${MC_CONFIG_DIR}"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for Minio service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/accesskey) ; SECRET=$(cat /config/secretkey) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to Minio server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} config host add myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # checkBucketExists ($bucket)
    # Check if the bucket exists, by using the exit code of `mc ls`
    checkBucketExists() {
      BUCKET=$1
      CMD=$(${MC} ls myminio/$BUCKET > /dev/null 2>&1)
      return $?
    }
    
    # createBucket ($bucket, $policy, $purge)
    # Ensure bucket exists, purging if asked to
    createBucket() {
      BUCKET=$1
      POLICY=$2
      PURGE=$3
    
      # Purge the bucket, if set & exists
      # Since PURGE is user input, check explicitly for `true`
      if [ $PURGE = true ]; then
        if checkBucketExists $BUCKET ; then
          echo "Purging bucket '$BUCKET'."
          set +e ; # don't exit if this fails
          ${MC} rm -r --force myminio/$BUCKET
          set -e ; # reset `e` as active
        else
          echo "Bucket '$BUCKET' does not exist, skipping purge."
        fi
      fi
    
      # Create the bucket if it does not exist
      if ! checkBucketExists $BUCKET ; then
        echo "Creating bucket '$BUCKET'"
        ${MC} mb myminio/$BUCKET
      else
        echo "Bucket '$BUCKET' already exists."
      fi
    
      # At this point, the bucket should exist, skip checking for existence
      # Set policy on the bucket
      echo "Setting policy of bucket '$BUCKET' to '$POLICY'."
      ${MC} policy set $POLICY myminio/$BUCKET
    }
    
    # Try connecting to Minio instance
    scheme=http
    connectToMinio $scheme
---
# Source: hkube/charts/redis-ha/templates/redis-ha-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hkube-redis-ha-configmap
  labels:
    heritage: Helm
    release: hkube
    chart: redis-ha-3.6.3
    app: hkube-redis-ha
data:
  redis.conf: |
    dir "/data"
    maxmemory 0
    maxmemory-policy volatile-lru
    min-slaves-max-lag 5
    min-slaves-to-write 1
    rdbchecksum yes
    rdbcompression yes
    repl-diskless-sync yes
    save 900 1

  sentinel.conf: |
    dir "/data"
    sentinel down-after-milliseconds mymaster 10000
    sentinel failover-timeout mymaster 180000
    sentinel parallel-syncs mymaster 5

  init.sh: |
    HOSTNAME="$(hostname)"
    INDEX="${HOSTNAME##*-}"
    MASTER="$(redis-cli -h hkube-redis-ha -p 26379 sentinel get-master-addr-by-name mymaster | grep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}')"
    MASTER_GROUP="mymaster"
    QUORUM="2"
    REDIS_CONF=/data/conf/redis.conf
    REDIS_PORT=6379
    SENTINEL_CONF=/data/conf/sentinel.conf
    SENTINEL_PORT=26379
    SERVICE=hkube-redis-ha
    set -eu

    sentinel_update() {
        echo "Updating sentinel config"
        eval MY_SENTINEL_ID="\${SENTINEL_ID_$INDEX}"
        sed -i "1s/^/sentinel myid $MY_SENTINEL_ID\\n/" "$SENTINEL_CONF"
        sed -i "2s/^/sentinel monitor $MASTER_GROUP $1 $REDIS_PORT $QUORUM \\n/" "$SENTINEL_CONF"
        echo "sentinel announce-ip $ANNOUNCE_IP" >> $SENTINEL_CONF
        echo "sentinel announce-port $SENTINEL_PORT" >> $SENTINEL_CONF
    }

    redis_update() {
        echo "Updating redis config"
        echo "slaveof $1 $REDIS_PORT" >> "$REDIS_CONF"
        echo "slave-announce-ip $ANNOUNCE_IP" >> $REDIS_CONF
        echo "slave-announce-port $REDIS_PORT" >> $REDIS_CONF
    }

    copy_config() {
        cp /readonly-config/redis.conf "$REDIS_CONF"
        cp /readonly-config/sentinel.conf "$SENTINEL_CONF"
    }

    setup_defaults() {
        echo "Setting up defaults"
        if [ "$INDEX" = "0" ]; then
            echo "Setting this pod as the default master"
            redis_update "$ANNOUNCE_IP"
            sentinel_update "$ANNOUNCE_IP"
            sed -i "s/^.*slaveof.*//" "$REDIS_CONF"
        else
            DEFAULT_MASTER="$(getent hosts "$SERVICE-announce-0" | awk '{ print $1 }')"
            if [ -z "$DEFAULT_MASTER" ]; then
                echo "Unable to resolve host"
                exit 1
            fi
            echo "Setting default slave config.."
            redis_update "$DEFAULT_MASTER"
            sentinel_update "$DEFAULT_MASTER"
        fi
    }

    find_master() {
        echo "Attempting to find master"
        if [ "$(redis-cli -h "$MASTER" ping)" != "PONG" ]; then
           echo "Can't ping master, attempting to force failover"
           if redis-cli -h "$SERVICE" -p "$SENTINEL_PORT" sentinel failover "$MASTER_GROUP" | grep -q 'NOGOODSLAVE' ; then 
               setup_defaults
               return 0
           fi
           sleep 10
           MASTER="$(redis-cli -h $SERVICE -p $SENTINEL_PORT sentinel get-master-addr-by-name $MASTER_GROUP | grep -E '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}')"
           if [ "$MASTER" ]; then
               sentinel_update "$MASTER"
               redis_update "$MASTER"
           else
              echo "Could not failover, exiting..."
              exit 1
           fi
        else
            echo "Found reachable master, updating config"
            sentinel_update "$MASTER"
            redis_update "$MASTER"
        fi
    }

    mkdir -p /data/conf/

    echo "Initializing config.."
    copy_config

    ANNOUNCE_IP=$(getent hosts "$SERVICE-announce-$INDEX" | awk '{ print $1 }')
    if [ -z "$ANNOUNCE_IP" ]; then
        "Could not resolve the announce ip for this pod"
        exit 1
    elif [ "$MASTER" ]; then
        find_master
    else
        setup_defaults
    fi

    if [ "${AUTH:-}" ]; then
        echo "Setting auth values"
        ESCAPED_AUTH=$(echo "$AUTH" | sed -e 's/[\/&]/\\&/g');
        sed -i "s/replace-default-auth/${ESCAPED_AUTH}/" "$REDIS_CONF" "$SENTINEL_CONF"
    fi

    echo "Ready..."
---
# Source: hkube/charts/redis-ha/templates/redis-ha-healthchecks.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hkube-redis-ha-probes
  labels:
    heritage: Helm
    release: hkube
    chart: redis-ha-3.6.3
    app: hkube-redis-ha
data:
  check-quorum.sh: |
    #!/bin/sh
    set -eu
    MASTER_GROUP="mymaster"
    SENTINEL_PORT=26379
    REDIS_PORT=6379
    NUM_SLAVES=$(redis-cli -p "$SENTINEL_PORT" sentinel master mymaster | awk '/num-slaves/{getline; print}')
    MIN_SLAVES=1

    if [ "$1" = "$SENTINEL_PORT" ]; then
        if redis-cli -p "$SENTINEL_PORT" sentinel ckquorum "$MASTER_GROUP" | grep -q NOQUORUM ; then
            echo "ERROR: NOQUORUM. Sentinel quorum check failed, not enough sentinels found"
            exit 1
        fi
    elif [ "$1" = "$REDIS_PORT" ]; then
        if [ "$MIN_SLAVES" -gt "$NUM_SLAVES" ]; then
            echo "Could not find enough replicating slaves. Needed $MIN_SLAVES but found $NUM_SLAVES"
            exit 1
        fi
    fi
    sh /probes/readiness.sh "$1"

  readiness.sh: |
    #!/bin/sh
    set -eu
    CHECK_SERVER="$(redis-cli -p "$1" ping)"

    if [ "$CHECK_SERVER" != "PONG" ]; then
        echo "Server check failed with: $CHECK_SERVER"
        exit 1
    fi

  liveness.sh: |
    #!/bin/sh
    set -eu
    CHECK_SERVER="$(redis-cli -p "$1" ping)"

    if [ "$CHECK_SERVER" != "PONG" ] && [ "$CHECK_SERVER" != "LOADING Redis is loading the dataset in memory" ]; then
        echo "Server check failed with: $CHECK_SERVER"
        exit 1
    fi
---
# Source: hkube/templates/configmap.yaml
apiVersion: v1
data:
  registry.json: |
        {"registry":"" }
  clusterOptions.json: |
        {
              "useNodeSelector": false, 
              "ingressHost": "",
              "ingressPrefix": "",
              "ingressUseRegex": true
        }
  versions.json: |
        {
            "systemVersion": "v1.3.7",
            "fullSystemVersion": "v1.3.7-1591537680069",
            "versions": [
                  {
                        "project": "api-server",
                        "tag": "v1.3.4",
                        "image": "hkube/api-server"
                  },
                  {
                        "project": "worker",
                        "tag": "v1.3.1",
                        "image": "hkube/worker"
                  },
                  {
                        "project": "pipeline-driver",
                        "tag": "v1.3.2",
                        "image": "hkube/pipeline-driver"
                  },
                  {
                        "project": "algorunner",
                        "tag": "v1.3.2",
                        "image": "hkube/algorunner"
                  },
                  {
                        "project": "simulator",
                        "tag": "v1.3.3",
                        "image": "hkube/simulator"
                  },
                  {
                        "project": "algorithm-example",
                        "tag": "v1.3.1",
                        "image": "hkube/algorithm-example"
                  },
                  {
                        "project": "monitor-server",
                        "tag": "v1.3.3",
                        "image": "hkube/monitor-server"
                  },
                  {
                        "project": "algorithm-queue",
                        "tag": "v1.3.1",
                        "image": "hkube/algorithm-queue"
                  },
                  {
                        "project": "resource-manager",
                        "tag": "v1.3.1",
                        "image": "hkube/resource-manager"
                  },
                  {
                        "project": "task-executor",
                        "tag": "v1.3.3",
                        "image": "hkube/task-executor"
                  },
                  {
                        "project": "trigger-service",
                        "tag": "v1.3.1",
                        "image": "hkube/trigger-service"
                  },
                  {
                        "project": "algorithm-operator",
                        "tag": "v1.3.1",
                        "image": "hkube/algorithm-operator"
                  },
                  {
                        "project": "pipeline-driver-queue",
                        "tag": "v1.3.1",
                        "image": "hkube/pipeline-driver-queue"
                  },
                  {
                        "project": "clean-old-jobs",
                        "tag": "v1.3.1",
                        "image": "hkube/clean-old-jobs"
                  },
                  {
                        "project": "cpu-load-algorithm",
                        "tag": "v1.3.1",
                        "image": "hkube/cpu-load-algorithm"
                  },
                  {
                        "project": "storage-cleaner",
                        "tag": "v1.3.2",
                        "image": "hkube/storage-cleaner"
                  },
                  {
                        "project": "pipeline-cleaner",
                        "tag": "v1.3.1",
                        "image": "hkube/pipeline-cleaner"
                  },
                  {
                        "project": "etcd-cleaner",
                        "tag": "v1.3.4",
                        "image": "hkube/etcd-cleaner"
                  },
                  {
                        "project": "caching-service",
                        "tag": "v1.3.1",
                        "image": "hkube/caching-service"
                  },
                  {
                        "project": "algorithm-builder",
                        "tag": "v1.3.1",
                        "image": "hkube/algorithm-builder"
                  },
                  {
                        "project": "kaniko",
                        "tag": "v1.3.1",
                        "image": "hkube/kaniko"
                  },
                  {
                        "project": "oc-builder",
                        "tag": "v1.3.0",
                        "image": "hkube/oc-builder"
                  },
                  {
                        "project": "algorithm-example-python",
                        "tag": "v1.3.1",
                        "image": "hkube/algorithm-example-python"
                  },
                  {
                        "project": "nodejs-env",
                        "tag": "v1.3.1",
                        "image": "hkube/nodejs-env"
                  },
                  {
                        "project": "python-env",
                        "tag": "v1.3.1",
                        "image": "hkube/python-env"
                  },
                  {
                        "project": "tensorboard",
                        "tag": "v1.3.1",
                        "image": "hkube/tensorboard"
                  },
                  {
                        "project": "storage-migration",
                        "tag": "v1.3.1",
                        "image": "hkube/storage-migration"
                  }

            ]
            }
kind: ConfigMap
metadata:
  name: hkube-versions
  namespace: default
---
# Source: hkube/templates/configmaps/algorithm-operator-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: algorithm-operator-configmap
data: 
  NODE_ENV: production
  DEFAULT_STORAGE: "s3"
  STORAGE_BINARY: "false"
  BASE_FS_ADAPTER_DIRECTORY: "/hkubedata"
  METRICS_PORT: "3000"
  CLUSTER_NAME: "cluster.local"
  BUILD_MODE: "kaniko"
  ALGORITHM_QUEUE_INTERVAL: "200"
  BOARDS_TIMEOUT: "10800"
  ALGORITHM_QUEUE_PRODUCER_UPDATE_INTERVAL : "1000"
---
# Source: hkube/templates/configmaps/api-server-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: api-server-configmap
data:
  NODE_ENV: production
  BASE_URL_PATH: "/hkube/api-server"
  DEFAULT_STORAGE: "s3"
  STORAGE_BINARY: "false"  
  METRICS_PORT: "3001"
  PORT: "3000"
  PIPELINE_DRIVER_CPU: "0.2"
  PIPELINE_DRIVER_MEM: "2048"
  CLUSTER_NAME: "cluster.local"
---
# Source: hkube/templates/configmaps/caching-service-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: caching-service-configmap
data:
  NODE_ENV: production
  DEFAULT_STORAGE: "s3"
  STORAGE_BINARY: "false"  
  PORT: "9005"
  CLUSTER_NAME: "cluster.local"
---
# Source: hkube/templates/configmaps/clean-old-jobs-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: clean-old-jobs-configmap
data:
  NODE_ENV: production
  MAX_COMPLETED_JOB_AGE_HOURS: "0.0166"
  MAX_FAILED_JOB_AGE_HOURS: "0.0166"
  MAX_PENDING_JOB_AGE_HOURS: "0.0166"
  CLEAN_CRON: "*/1 * * * *"
  HKUBE_LOG_LEVEL: "2"
---
# Source: hkube/templates/configmaps/etcd-cleaner-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: etcd-cleaner-configmap
data:
  OBJECT_EXPIRATION_DAYS: "5"
---
# Source: hkube/templates/configmaps/monitor-server-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: monitor-server-configmap
data:
  NODE_ENV: production
  DEFAULT_STORAGE: "s3"
  STORAGE_BINARY: "false"
  CLUSTER_NAME: "cluster.local"
  JAEGER_QUERY_SERVICE_HOST: ""
  JAEGER_QUERY_SERVICE_PORT: ""
---
# Source: hkube/templates/configmaps/pipeline-driver-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: pipeline-driver-configmap
data:
  NODE_ENV: production
  DEFAULT_STORAGE: s3   
  METRICS_PORT: "3001"
  CLUSTER_NAME: "cluster.local"
---
# Source: hkube/templates/configmaps/pipeline-driver-queue-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: pipeline-driver-queue-configmap
data:
  NODE_ENV: production
  KUBE_LOG_LEVEL: "2"
  METRICS_PORT: "3000"
  CHECK_QUEUE_INTERVAL: "500"
---
# Source: hkube/templates/configmaps/resource-manager-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: resource-manager-configmap
data:
  NODE_ENV: production
  HKUBE_LOG_LEVEL: "2"
  ALGORITHMS_THRESHOLD_CPU: "0.9"
  ALGORITHMS_THRESHOLD_MEM: "0.9"
  DRIVERS_THRESHOLD_CPU: "0.1"
  DRIVERS_THRESHOLD_MEM: "1"
  METRICS_PORT: "3000"
  PROMETHEUS_ENDPOINT: http://monitoring-prometheus-server.monitoring:9090/api/v1
---
# Source: hkube/templates/configmaps/simulator-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: simulator-configmap
data:
  MONITOR_BACKEND_PATH: "/hkube/monitor-server/"
  MONITOR_BACKEND_PATH_SOCKETIO: "/hkube/monitor-server/socket.io"
  MONITOR_BACKEND_USE_LOCATION: "true"
  MONITOR_BACKEND_HOST: ""
  MONITOR_BACKEND_PORT: ""
  MONITOR_BACKEND_IS_SECURE: ""
  
  BOARD_PATH: "/hkube/board"
  BOARD_USE_LOCATION: "true"
  BOARD_HOST: ""
  BOARD_PORT: ""
---
# Source: hkube/templates/configmaps/storage-cleaner-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: storage-cleaner-configmap
data:
  TEMP_OBJECT_EXPIRATION_DAYS: "5"
  RESULT_OBJECT_EXPIRATION_DAYS: "20"
  DEFAULT_STORAGE: s3
  CLUSTER_NAME: "cluster.local"
---
# Source: hkube/templates/configmaps/task-executor-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: task-executor-configmap
data:
  NODE_ENV: production
  DEFAULT_STORAGE: "s3"
  WORKER_SOCKET_MAX_PAYLOAD_BYTES: "1e+09"
  STORAGE_BINARY: "false"
  BASE_FS_ADAPTER_DIRECTORY: "/hkubedata"
  METRICS_PORT: "3000"
  IS_NAMESPACED: "false"
  IS_PRIVILEGED: "true"
  HAS_NODE_LIST: "false"
  PIPELINE_DRIVERS_AMOUNT: "30"
  CLUSTER_NAME: "cluster.local"
---
# Source: hkube/templates/configmaps/trigger-service-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: trigger-service-configmap
data:
  NODE_ENV: production
  METRICS_PORT: "3000"
---
# Source: hkube/charts/minio/templates/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: hkube-minio
  labels:
    app: minio
    chart: minio-5.0.23
    release: hkube
    heritage: Helm
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "10Gi"
---
# Source: hkube/templates/cleanup-role.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: cleanup-clusterrole
  labels:
    group: hkube
    app: cleanup
    core: "true"
rules:
- apiGroups:
  - etcd.database.coreos.com
  resources:
  - etcdclusters
  - etcdbackups
  - etcdrestores
  verbs:
  - "*"
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - customresourcedefinitions
  verbs:
  - "*"
---
# Source: hkube/templates/resource-manager-role.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: resource-manager-clusterrole
  labels:
    group: hkube
    app: resource-manager
    core: "true"
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "watch", "list"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
---
# Source: hkube/templates/task-executor-role.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: task-executor-clusterrole
  labels:
    group: hkube
    app: task-executor
    core: "true"
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "watch", "list"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
---
# Source: hkube/templates/task-executor-role.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: worker-clusterrole
  labels:
    group: hkube
    app: task-executor
    core: "true"
rules:
- apiGroups: ["apiextensions.k8s.io"]
  resources: ["customresourcedefinitions"]
  verbs: ["get"]
---
# Source: hkube/templates/cleanup-role.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: cleanup-clusterrolebinding
  labels:
    group: hkube
    app: cleanup
    core: "true"
subjects:
- kind: ServiceAccount
  name: cleanup-serviceaccount
  namespace: default
roleRef:
  kind: ClusterRole
  name: cleanup-clusterrole
  apiGroup: rbac.authorization.k8s.io
---
# Source: hkube/templates/resource-manager-role.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: resource-manager-clusterrolebinding
  labels:
    group: hkube
    app: resource-manager
    core: "true"
subjects:
- kind: ServiceAccount
  name: resource-manager-serviceaccount
  namespace: default
roleRef:
  kind: ClusterRole
  name: resource-manager-clusterrole
  apiGroup: rbac.authorization.k8s.io
---
# Source: hkube/templates/task-executor-role.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: task-executor-clusterrolebinding
  labels:
    group: hkube
    app: task-executor
    core: "true"
subjects:
- kind: ServiceAccount
  name: task-executor-serviceaccount
  namespace: default
roleRef:
  kind: ClusterRole
  name: task-executor-clusterrole
  apiGroup: rbac.authorization.k8s.io
---
# Source: hkube/templates/task-executor-role.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: worker-clusterrolebinding
  labels:
    group: hkube
    app: task-executor
    core: "true"
subjects:
- kind: ServiceAccount
  name: worker-serviceaccount
  namespace: default
roleRef:
  kind: ClusterRole
  name: worker-clusterrole
  apiGroup: rbac.authorization.k8s.io
---
# Source: hkube/charts/minio/templates/post-install-prometheus-metrics-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: hkube-minio-update-prometheus-secret
  labels:
    app: minio-update-prometheus-secret
    chart: minio-5.0.23
    release: hkube
    heritage: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - create
      - update
      - patch
    resourceNames:
      - hkube-minio-prometheus
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
  - apiGroups:
      - monitoring.coreos.com
    resources:
      - servicemonitors
    verbs:
      - get
    resourceNames:
      - hkube-minio
---
# Source: hkube/charts/redis-ha/templates/redis-ha-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: hkube-redis-ha
  labels:
    heritage: Helm
    release: hkube
    chart: redis-ha-3.6.3
    app: hkube-redis-ha
rules:
- apiGroups:
    - ""
  resources:
    - endpoints
  verbs:
    - get
---
# Source: hkube/templates/algorithm-builder-role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: algorithm-builder
  labels:
    group: hkube
    app: algorithm-builder
    core: "true"
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["get"]
---
# Source: hkube/templates/algorithm-operator-role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: algorithm-operator
  labels:
    group: hkube
    app: algorithm-operator
    core: "true"
rules:
- apiGroups: [""]
  resources: ["pods", "secrets"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["get"]
- apiGroups: ["extensions", "apps"]
  resources: ["deployments"]
  verbs: [create, delete, deletecollection, get, list, patch, update, watch]
- apiGroups: ["batch"]
  resources: ["jobs"]
  verbs: [create, delete, deletecollection, get, list, patch, update, watch]
- apiGroups: ["extensions"]
  resources: ["ingresses"]
  verbs: [create, delete, deletecollection, get, list, patch, update, watch]
- apiGroups:
  - ""
  resources:
  - services
  verbs: [create, delete, deletecollection, get, list, patch, update, watch]
---
# Source: hkube/templates/cleanup-role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: cleanup
  labels:
    group: hkube
    app: cleanup
    core: "true"
rules:
- apiGroups: [""]
  resources: ["pods", "secrets", "services"]
  verbs: ["get", "list", "delete"]
- apiGroups: ["batch"]
  resources: ["jobs"]
  verbs: ["get", "list", "delete"]
- apiGroups: ["extensions", "apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "delete"]
- apiGroups: ["extensions"]
  resources: ["ingresses"]
  verbs: ["get", "list", "delete"]
---
# Source: hkube/templates/job-cleaner-role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: clean-old-jobs
  labels:
    group: hkube
    app: clean-old-jobs
    core: "true"
rules:
- apiGroups: [""]
  resources: ["pods", "endpoints"]
  verbs: ["get", "list", "watch", "delete"]
- apiGroups: ["batch"]
  resources: ["jobs"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
# Source: hkube/templates/monitor-server-role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: monitor-server
  labels:
    group: hkube
    app: monitor-server
    core: "true"
rules:
- apiGroups: [""]
  resources: ["pods","pods/log"]
  verbs: ["get", "list", "watch"]
---
# Source: hkube/templates/resource-manager-role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: resource-manager
  labels:
    group: hkube
    app: resource-manager-role
    core: "true"
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
---
# Source: hkube/templates/task-executor-role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: task-executor
  labels:
    group: hkube
    app: task-executor
    core: "true"
rules:
- apiGroups: [""]
  resources: ["pods", "resourcequotas"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["get"]
- apiGroups: ["batch"]
  resources: ["jobs"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
# Source: hkube/templates/task-executor-role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: worker-role
  labels:
    group: hkube
    app: task-executor
    core: "true"
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get"]
- apiGroups: ["batch"]
  resources: ["jobs"]
  verbs: ["delete"]
# - apiGroups: ["apiextensions.k8s.io"]
#   resources: ["customresourcedefinitions"]
#   verbs: ["get"]
---
# Source: hkube/charts/minio/templates/post-install-prometheus-metrics-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: hkube-minio-update-prometheus-secret
  labels:
    app: minio-update-prometheus-secret
    chart: minio-5.0.23
    release: hkube
    heritage: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: hkube-minio-update-prometheus-secret
subjects:
  - kind: ServiceAccount
    name: hkube-minio-update-prometheus-secret
    namespace: default
---
# Source: hkube/charts/redis-ha/templates/redis-ha-rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: hkube-redis-ha
  labels:
    heritage: Helm
    release: hkube
    chart: redis-ha-3.6.3
    app: hkube-redis-ha
subjects:
- kind: ServiceAccount
  name: hkube-redis-ha
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: hkube-redis-ha
---
# Source: hkube/templates/algorithm-builder-role.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: algorithm-builder
  namespace: default
  labels:
    group: hkube
    app: algorithm-builder
    core: "true"
subjects:
- kind: ServiceAccount
  name: algorithm-builder-serviceaccount
roleRef:
  kind: Role
  name: algorithm-builder
  apiGroup: rbac.authorization.k8s.io
---
# Source: hkube/templates/algorithm-operator-role.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: algorithm-operator
  namespace: default
  labels:
    group: hkube
    app: algorithm-operator
    core: "true"
subjects:
- kind: ServiceAccount
  name: algorithm-operator-serviceaccount
roleRef:
  kind: Role
  name: algorithm-operator
  apiGroup: rbac.authorization.k8s.io
---
# Source: hkube/templates/cleanup-role.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: cleanup
  namespace: default
  labels:
    group: hkube
    app: cleanup
    core: "true"
subjects:
- kind: ServiceAccount
  name: cleanup-serviceaccount
roleRef:
  kind: Role
  name: cleanup
  apiGroup: rbac.authorization.k8s.io
---
# Source: hkube/templates/job-cleaner-role.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: clean-old-jobs
  namespace: default
  labels:
    group: hkube
    app: clean-old-jobs
    core: "true"
subjects:
- kind: ServiceAccount
  name: clean-old-jobs
roleRef:
  kind: Role
  name: clean-old-jobs
  apiGroup: rbac.authorization.k8s.io
---
# Source: hkube/templates/monitor-server-role.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: monitor-server
  namespace: default
  labels:
    group: hkube
    app: monitor-server
    core: "true"
subjects:
- kind: ServiceAccount
  name: monitor-server-serviceaccount
roleRef:
  kind: Role
  name: monitor-server
  apiGroup: rbac.authorization.k8s.io
---
# Source: hkube/templates/resource-manager-role.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: resource-manager
  namespace: default
  labels:
    group: hkube
    app: resource-manager
    core: "true"
subjects:
- kind: ServiceAccount
  name: resource-manager-serviceaccount
roleRef:
  kind: Role
  name: resource-manager
  apiGroup: rbac.authorization.k8s.io
---
# Source: hkube/templates/task-executor-role.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: task-executor
  namespace: default
  labels:
    group: hkube
    app: task-executor
    core: "true"
subjects:
- kind: ServiceAccount
  name: task-executor-serviceaccount
roleRef:
  kind: Role
  name: task-executor
  apiGroup: rbac.authorization.k8s.io
---
# Source: hkube/templates/task-executor-role.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: worker-rolebinding
  namespace: default
  labels:
    group: hkube
    app: task-executor
    core: "true"
subjects:
- kind: ServiceAccount
  name: worker-serviceaccount
roleRef:
  kind: Role
  name: worker-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: hkube/charts/etcd/templates/etcd-ui.yml
kind: Service
apiVersion: v1
metadata:
  name: etcd-ui
  labels:
    app: etcd-ui
    release: hkube
    heritage: Helm
    third-party: "true"
spec:
  selector:
    app: etcd-ui
  ports:
    - name: server
      protocol: TCP
      port: 8080
      targetPort: 8080
---
# Source: hkube/charts/etcd/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
  name: hkube-etcd
  labels:
    heritage: "Helm"
    release: "hkube"
    chart: "etcd-0.7.2007"
    app: etcd
spec:
  ports:
  - port: 2380
    name: etcd-server
  - port: 2379
    name: etcd-client
  clusterIP: None
  selector:
    app: etcd
    release: "hkube"
---
# Source: hkube/charts/etcd/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
  name: etcd-client
  labels:
    heritage: "Helm"
    release: "hkube"
    chart: "etcd-0.7.2007"
    app: etcd
spec:
  ports:
  - port: 2380
    name: etcd-server
  - port: 2379
    name: etcd-client
  selector:
    app: etcd
    release: "hkube"
---
# Source: hkube/charts/jaeger/charts/cassandra/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: hkube-cassandra
  labels:
    app: cassandra
    chart: cassandra-0.15.0
    release: hkube
    heritage: Helm
spec:
  clusterIP: None
  type: ClusterIP
  ports:
  - name: intra
    port: 7000
    targetPort: 7000
  - name: tls
    port: 7001
    targetPort: 7001
  - name: jmx
    port: 7199
    targetPort: 7199
  - name: cql
    port: 9042
    targetPort: 9042
  - name: thrift
    port: 9160
    targetPort: 9160
  selector:
    app: cassandra
    release: hkube
---
# Source: hkube/charts/jaeger/templates/agent-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: jaeger-agent
  labels:
    helm.sh/chart: jaeger-0.27.2004
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: hkube
    app.kubernetes.io/version: "1.17.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: agent
spec:
  ports:
  - name: zipkin-compact
    port: 5775
    protocol: UDP
    targetPort: zipkin-compact
  - name: jaeger-compact
    port: 6831
    protocol: UDP
    targetPort: jaeger-compact
  - name: jaeger-binary
    port: 6832
    protocol: UDP
    targetPort: jaeger-binary
  - name: http
    port: 5778
    protocol: TCP
    targetPort: http
  - name: admin
    port: 14271
    protocol: TCP
    targetPort: admin
  type: ClusterIP
  selector:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: hkube
    app.kubernetes.io/component: agent
---
# Source: hkube/charts/jaeger/templates/collector-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: jaeger-collector
  labels:
    helm.sh/chart: jaeger-0.27.2004
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: hkube
    app.kubernetes.io/version: "1.17.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: collector
spec:
  ports:
  - name: grpc
    port: 14250
    protocol: TCP
    targetPort: grpc
  - name: tchannel
    port: 14267
    protocol: TCP
    targetPort: tchannel
  - name: http
    port: 14268
    protocol: TCP
    targetPort: http
  - name: admin
    port: 14269
    targetPort: admin
  selector:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: hkube
    app.kubernetes.io/component: collector
  type: ClusterIP
---
# Source: hkube/charts/jaeger/templates/query-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: jaeger-query
  labels:
    helm.sh/chart: jaeger-0.27.2004
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: hkube
    app.kubernetes.io/version: "1.17.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: query
spec:
  ports:
  - name: query
    port: 80
    protocol: TCP
    targetPort: query
  - name: admin
    port: 16687
    protocol: TCP
    targetPort: admin
  selector:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: hkube
    app.kubernetes.io/component: query
  type: ClusterIP
---
# Source: hkube/charts/minio/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: hkube-minio
  labels:
    app: minio
    chart: minio-5.0.23
    release: hkube
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 9000
      protocol: TCP
      targetPort: 9000
  selector:
    app: minio
    release: hkube
---
# Source: hkube/charts/redis-ha/templates/redis-ha-announce-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: hkube-redis-ha-announce-0
  labels:
    app: redis-ha
    heritage: "Helm"
    release: "hkube"
    chart: redis-ha-3.6.3
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  publishNotReadyAddresses: true
  type: ClusterIP
  ports:
  - name: server
    port: 6379
    protocol: TCP
    targetPort: redis
  - name: sentinel
    port: 26379
    protocol: TCP
    targetPort: sentinel
  selector:
    release: hkube
    app: redis-ha
    "statefulset.kubernetes.io/pod-name": hkube-redis-ha-server-0
---
# Source: hkube/charts/redis-ha/templates/redis-ha-announce-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: hkube-redis-ha-announce-1
  labels:
    app: redis-ha
    heritage: "Helm"
    release: "hkube"
    chart: redis-ha-3.6.3
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  publishNotReadyAddresses: true
  type: ClusterIP
  ports:
  - name: server
    port: 6379
    protocol: TCP
    targetPort: redis
  - name: sentinel
    port: 26379
    protocol: TCP
    targetPort: sentinel
  selector:
    release: hkube
    app: redis-ha
    "statefulset.kubernetes.io/pod-name": hkube-redis-ha-server-1
---
# Source: hkube/charts/redis-ha/templates/redis-ha-announce-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: hkube-redis-ha-announce-2
  labels:
    app: redis-ha
    heritage: "Helm"
    release: "hkube"
    chart: redis-ha-3.6.3
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  publishNotReadyAddresses: true
  type: ClusterIP
  ports:
  - name: server
    port: 6379
    protocol: TCP
    targetPort: redis
  - name: sentinel
    port: 26379
    protocol: TCP
    targetPort: sentinel
  selector:
    release: hkube
    app: redis-ha
    "statefulset.kubernetes.io/pod-name": hkube-redis-ha-server-2
---
# Source: hkube/charts/redis-ha/templates/redis-ha-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: hkube-redis-ha
  labels:
    app: redis-ha
    heritage: "Helm"
    release: "hkube"
    chart: redis-ha-3.6.3
  annotations:
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: server
    port: 6379
    protocol: TCP
    targetPort: redis
  - name: sentinel
    port: 26379
    protocol: TCP
    targetPort: sentinel
  selector:
    release: hkube
    app: redis-ha
---
# Source: hkube/charts/redis-ha/templates/redis-sentinel-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-sentinel
  labels:
    name: redis-ha-sentinel-svc
    role: service
    app: redis-ha
    heritage: "Helm"
    release: "hkube"
    chart: redis-ha-3.6.3
spec:
  ports:
    - port: 26379
      targetPort: 26379
  selector:
    app: redis-ha
    release: "hkube"
---
# Source: hkube/templates/algorithm-operator-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: algorithm-operator
  annotations:
    prometheus.io/scrape: "true"
  labels:
    app: algorithm-operator
    chart: algorithm-operator
    release: hkube
    heritage: Helm
    group: hkube
    core: "true"
spec:
  clusterIP: None
  type: "ClusterIP"
  ports:
    - port: 3000
      targetPort: 3000
      protocol: TCP
  selector:
    app: algorithm-operator
    release: hkube
---
# Source: hkube/templates/algorithm-queue-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: algorithm-queue-svc
  annotations:
    prometheus.io/scrape: "true"
  labels:
    app: algorithm-queue-svc
    chart: algorithm-queue-svc
    release: hkube
    heritage: Helm
    group: hkube
    core: "true"
spec:
  type: ClusterIP
  ports:
    - port: 3000
      targetPort: 3000
      protocol: TCP
  selector:
    metrics-group: 'algorithm-queue'
---
# Source: hkube/templates/api-server-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: api-server
  annotations:
    prometheus.io/scrape: "true"
  labels:
    app: api-server
    release: hkube
    heritage: Helm
    group: hkube
    core: "true"
spec:
  type: ClusterIP
  ports:
    - port: 3000
      targetPort: 3000
      protocol: TCP
      name: rest
    - port: 3001
      targetPort: 3001
      protocol: TCP
      name: metric
    
  selector:
    app: api-server
    release: api-server
---
# Source: hkube/templates/caching-service-service.yaml
kind: Service
apiVersion: v1
metadata:
  name: caching-service
  labels:
    app: caching-service
    group: hkube
    core: "true"
spec:
  selector:
    app: caching-service
  ports:
    - name: http
      port: 9005
      targetPort: 9005
---
# Source: hkube/templates/monitor-server-service.yaml
kind: Service
apiVersion: v1
metadata:
  name: monitor-server
  labels:
    app: monitor-server
    group: hkube
    core: "true"
spec:
  selector:
    app: monitor-server
  ports:
    - protocol: TCP
      port: 30010
      targetPort: 30010
---
# Source: hkube/templates/pipeline-driver-queue-service.yaml
kind: Service
apiVersion: v1
metadata:
  name: pipeline-driver-queue
  annotations:
    prometheus.io/scrape: "true"
  labels:
    app: pipeline-driver-queue
    group: hkube
    core: "true"
spec:
  selector:
    app: pipeline-driver-queue
  clusterIP: None
  ports:
    - name: metrics
      port: 3000
      targetPort: 3000
---
# Source: hkube/templates/pipeline-driver-service.yaml
kind: Service
apiVersion: v1
metadata:
  name: pipeline-driver
  annotations:
    prometheus.io/scrape: "true"
  labels:
    app: pipeline-driver
    group: hkube
    core: "true"
spec:
  selector:
    group: hkube
    metrics-group: pipeline-drivers
  clusterIP: None
  ports:
    - name: metrics
      port: 3001
      targetPort: 3001
---
# Source: hkube/templates/resource-manager-service.yaml
kind: Service
apiVersion: v1
metadata:
  name: resource-manager
  annotations:
    prometheus.io/scrape: "true"
  labels:
    app: resource-manager
    group: hkube
    core: "true"
spec:
  selector:
    app: resource-manager
  clusterIP: None
  ports:
    - name: metrics
      port: 3000
      targetPort: 3000
---
# Source: hkube/templates/simulator-service.yaml
kind: Service
apiVersion: v1
metadata:
  name: simulator
  labels:
    app: simulator
    group: hkube
    core: "true"
spec:
  selector:
    app: simulator
  ports:
    - protocol: TCP
      port: 9050
      targetPort: 9050
---
# Source: hkube/templates/task-executor-service.yaml
kind: Service
apiVersion: v1
metadata:
  name: task-executor
  annotations:
    prometheus.io/scrape: "true"
  labels:
    app: task-executor
    group: hkube
    core: "true"
spec:
  selector:
    app: task-executor
  clusterIP: None
  ports:
    - name: metrics
      port: 3000
      targetPort: 3000
---
# Source: hkube/templates/trigger-service-service.yaml
kind: Service
apiVersion: v1
metadata:
  name: trigger-service
  annotations:
    prometheus.io/scrape: "true"
  labels:
    app: trigger-service
    group: hkube
    core: "true"
spec:
  selector:
    app: trigger-service
  clusterIP: None
  ports:
    - name: metrics
      port: 3000
      targetPort: 3000
---
# Source: hkube/templates/worker-svc.yaml
kind: Service
apiVersion: v1
metadata:
  name: worker
  annotations:
    prometheus.io/scrape: "true"
  labels:
    app: worker
    group: hkube
    core: "true"
spec:
  selector:
    metrics-group: workers
    group: hkube
  clusterIP: None
  ports:
    - name: metrics
      port: 3001
      targetPort: 3001
---
# Source: hkube/charts/jaeger/templates/agent-ds.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: jaeger-agent
  labels:
    helm.sh/chart: jaeger-0.27.2004
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: hkube
    app.kubernetes.io/version: "1.17.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: agent
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: jaeger
      app.kubernetes.io/instance: hkube
      app.kubernetes.io/component: agent
  template:
    metadata:
      labels:
        app.kubernetes.io/name: jaeger
        app.kubernetes.io/instance: hkube
        app.kubernetes.io/component: agent
    spec:
      securityContext:
        {}
      dnsPolicy: ClusterFirst
      serviceAccountName: jaeger-agent
      containers:
      - name: jaeger-agent
        securityContext:
          {}
        image: jaegertracing/jaeger-agent:1.17.1
        imagePullPolicy: IfNotPresent
        args:
        env:
        - name: REPORTER_GRPC_HOST_PORT
          value: jaeger-collector:14250
        ports:
        - name: zipkin-compact
          containerPort: 5775
          protocol: UDP
          hostPort: 5775
        - name: jaeger-compact
          containerPort: 6831
          protocol: UDP
          hostPort: 6831
        - name: jaeger-binary
          containerPort: 6832
          protocol: UDP
          hostPort: 6832
        - name: http
          containerPort: 5778
          protocol: TCP
          hostPort: 5778
        - name: admin
          containerPort: 14271
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /
            port: admin
        readinessProbe:
          httpGet:
            path: /
            port: admin
        resources:
          {}
        volumeMounts:
      volumes:
---
# Source: hkube/templates/busybox-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  labels:
    app: busybox
    group: hkube
    thirdparty: "true"
spec:
  containers:
  - image: "busybox:latest"
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
    name: busybox
    resources:
      limits:
        cpu: 50m
        memory: 250Mi
  terminationGracePeriodSeconds: 5  
  restartPolicy: Always
---
# Source: hkube/charts/etcd/templates/etcd-ui.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: etcd-ui
  labels:
    app: etcd-ui
    group: hkube
    third-party: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: etcd-ui
  template:
    metadata:
      labels:
        app: etcd-ui
        group: hkube
    spec:
      containers:
        - name: etcd-ui
          image: "hkube/etcd-ui:v1.0.3"
          env: 
            - name: HOST
              value: "0.0.0.0"
          ports:
            - containerPort: 8080
---
# Source: hkube/charts/jaeger/templates/collector-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger-collector
  labels:
    helm.sh/chart: jaeger-0.27.2004
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: hkube
    app.kubernetes.io/version: "1.17.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: collector
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: jaeger
      app.kubernetes.io/instance: hkube
      app.kubernetes.io/component: collector
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        checksum/config-env: 75a11da44c802486bc6f65640aa48a730f0f684c5c07a42ba3cd1735eb3fb070
      labels:
        app.kubernetes.io/name: jaeger
        app.kubernetes.io/instance: hkube
        app.kubernetes.io/component: collector
    spec:
      securityContext:
        {}
      serviceAccountName: jaeger-collector
      containers:
      - name: jaeger-collector
        securityContext:
          {}
        image: jaegertracing/jaeger-collector:1.17.1
        imagePullPolicy: IfNotPresent
        args:
          
        env:
          - name: SPAN_STORAGE_TYPE
            value: cassandra
          - name: CASSANDRA_SERVERS
            value: hkube-cassandra
          - name: CASSANDRA_PORT
            value: "9042"
          
          - name: CASSANDRA_KEYSPACE
            value: jaeger_v1_test
          - name: CASSANDRA_USERNAME
            value: user
          - name: CASSANDRA_PASSWORD
            valueFrom:
              secretKeyRef:
                name: jaeger-cassandra
                key: password
        ports:
        - containerPort: 14250
          name: grpc
          protocol: TCP
        - containerPort: 14267
          name: tchannel
          protocol: TCP
        - containerPort: 14268
          name: http
          protocol: TCP
        - containerPort: 14269
          name: admin
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /
            port: admin
        livenessProbe:
          httpGet:
            path: /
            port: admin
        resources:
          {}
        volumeMounts:
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      volumes:
---
# Source: hkube/charts/jaeger/templates/query-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger-query
  labels:
    helm.sh/chart: jaeger-0.27.2004
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: hkube
    app.kubernetes.io/version: "1.17.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: query
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: jaeger
      app.kubernetes.io/instance: hkube
      app.kubernetes.io/component: query
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: jaeger
        app.kubernetes.io/instance: hkube
        app.kubernetes.io/component: query
    spec:
      securityContext:
        {}
      serviceAccountName: jaeger-query
      containers:
      - name: jaeger-query
        securityContext:
          {}
        image: jaegertracing/jaeger-query:1.17.1
        imagePullPolicy: IfNotPresent
        args:
          
        env:
          - name: SPAN_STORAGE_TYPE
            value: cassandra
          - name: CASSANDRA_SERVERS
            value: hkube-cassandra
          - name: CASSANDRA_PORT
            value: "9042"
          
          - name: CASSANDRA_KEYSPACE
            value: jaeger_v1_test
          - name: CASSANDRA_USERNAME
            value: user
          - name: CASSANDRA_PASSWORD
            valueFrom:
              secretKeyRef:
                name: jaeger-cassandra
                key: password
          - name: QUERY_BASE_PATH
            value: "/jaeger"
          - name: JAEGER_AGENT_PORT
            value: "6831"
        ports:
        - name: query
          containerPort: 16686
          protocol: TCP
        - name: admin
          containerPort: 16687
          protocol: TCP
        resources:
          {}
        volumeMounts:
        livenessProbe:
          httpGet:
            path: /
            port: admin
        readinessProbe:
          httpGet:
            path: /
            port: admin
      - name: jaeger-agent-sidecar
        securityContext:
          {}
        image: jaegertracing/jaeger-agent:1.17.1
        imagePullPolicy: IfNotPresent
        args:
        env:
        - name: REPORTER_GRPC_HOST_PORT
          value: jaeger-collector:14250
        ports:
        - name: admin
          containerPort: 14271
          protocol: TCP
        volumeMounts:
        livenessProbe:
          httpGet:
            path: /
            port: admin
        readinessProbe:
          httpGet:
            path: /
            port: admin
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      volumes:
---
# Source: hkube/charts/minio/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hkube-minio
  labels:
    app: minio
    chart: minio-5.0.23
    release: hkube
    heritage: Helm
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 100%
      maxUnavailable: 0
  selector:
    matchLabels:
      app: minio
      release: hkube
  template:
    metadata:
      name: hkube-minio
      labels:
        app: minio
        release: hkube
      annotations:
        rollme: "Q1lZb"
        checksum/secrets: caa5c310ee507296a6313c5c5f526d644075e7827640070dd8d33b9911e013eb
        checksum/config: aafaff8dac2be275d684b548f5133424247a211d6d75d7b3268d3ffab0636977
    spec:
      serviceAccountName: "hkube-minio"
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
      containers:
        - name: minio
          image: "minio/minio:RELEASE.2020-03-25T07-03-04Z"
          imagePullPolicy: IfNotPresent
          command: [ "/bin/sh",
          "-ce",
          "/usr/bin/docker-entrypoint.sh minio -S /etc/minio/certs/ server /export" ]
          volumeMounts:
            - name: export
              mountPath: /export
          ports:
            - name: http
              containerPort: 9000
          env:
            - name: MINIO_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: hkube-minio
                  key: accesskey
            - name: MINIO_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: hkube-minio
                  key: secretkey
          livenessProbe:
            httpGet:
              path: /minio/health/live
              port: http
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 30
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /minio/health/ready
              port: http
            initialDelaySeconds: 60
            periodSeconds: 15
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
      volumes:
        - name: export
          persistentVolumeClaim:
            claimName: hkube-minio
        - name: minio-user
          secret:
            secretName: hkube-minio
---
# Source: hkube/templates/algorithm-operator-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: algorithm-operator
  labels:
    app: algorithm-operator
    release: hkube
    heritage: Helm
    group: hkube
    core: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: algorithm-operator
  template:
    metadata:
      annotations:
        checksum/config: f22c1158c505af4e5dfe263928a45f07499e817c4b084b8d01bc40159065c514
      labels:
        app: algorithm-operator
        release: hkube
        group: hkube
    spec:
      serviceAccountName: algorithm-operator-serviceaccount
      containers:
        - name: algorithm-operator
          image: "hkube/algorithm-operator:v1.3.1"
          livenessProbe:
            httpGet:
              path: /healthz
              port: 5000
            initialDelaySeconds: 100
            periodSeconds: 60
            timeoutSeconds: 1
            failureThreshold: 3          
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 3000
              protocol: TCP
          env:
            - name: NODE_ENV
              valueFrom:
                configMapKeyRef:
                  name: algorithm-operator-configmap
                  key: NODE_ENV
            - name: METRICS_PORT
              valueFrom:
                configMapKeyRef:
                  name: algorithm-operator-configmap
                  key: METRICS_PORT
            - name: JAEGER_AGENT_SERVICE_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: DEFAULT_STORAGE
              valueFrom:
                configMapKeyRef:
                  name: algorithm-operator-configmap
                  key: DEFAULT_STORAGE
            - name: STORAGE_BINARY
              valueFrom:
                configMapKeyRef:
                  name: algorithm-operator-configmap
                  key: STORAGE_BINARY                  
            - name: BOARDS_TIMEOUT
              valueFrom:
                configMapKeyRef:
                  name: algorithm-operator-configmap
                  key: BOARDS_TIMEOUT
            - name: CLUSTER_NAME
              valueFrom:
                configMapKeyRef:
                  name: algorithm-operator-configmap
                  key: CLUSTER_NAME
            - name: BUILD_MODE
              valueFrom:
                configMapKeyRef:
                  name: algorithm-operator-configmap
                  key: BUILD_MODE
            - name: ALGORITHM_BUILDER_BUILDER_CPU
              value: 
            - name: ALGORITHM_BUILDER_BUILDER_MEMORY
              value: "1024"
            - name: ALGORITHM_BUILDER_MAIN_CPU
              value: "0.5"
            - name: ALGORITHM_BUILDER_MAIN_MEMORY
              value: "512"
            - name: RESOURCES_ENABLE
              value: "false"
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: s3-secret
                  key: awsKey
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: s3-secret
                  key: awsSecret
            - name: S3_ENDPOINT_URL
              valueFrom:
                secretKeyRef:
                  name: s3-secret
                  key: awsEndpointUrl              
            
          resources:
            requests:
              cpu: 100m
              memory: 512Mi
---
# Source: hkube/templates/api-server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
  labels:
    app: api-server
    release: hkube
    heritage: Helm
    group: hkube
    core: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: api-server
      release: api-server
  template:
    metadata:
      annotations:
        checksum/config: 6a2ee2802073d973ff619e09bf7a664b74c7d879eee976ac586ad6976dedb7ce
      labels:
        app: api-server
        release: api-server
        group: hkube
    spec:
      volumes:
        - name: uploads
          emptyDir: {}                                    
      containers:
        - name: api-server
          image: "hkube/api-server:v1.3.4"
          imagePullPolicy: IfNotPresent
          resources:
            requests:
              cpu: 100m
              memory: 512Mi 
          volumeMounts:
            - name: uploads
              mountPath: /hkube/api-server/uploads                                    
          ports:
            - containerPort: 3001
              protocol: TCP
            - containerPort: 3000
              protocol: TCP
          env:
            - name: METRICS_PORT
              valueFrom:
                configMapKeyRef:
                  name: api-server-configmap
                  key: METRICS_PORT
            - name: API_SERVER_REST_PORT
              valueFrom:
                configMapKeyRef:
                  name: api-server-configmap
                  key: PORT
            - name: NODE_ENV
              valueFrom:
                configMapKeyRef:
                  name: api-server-configmap
                  key: NODE_ENV
            - name: BASE_URL_PATH
              valueFrom:
                configMapKeyRef:
                  name: api-server-configmap
                  key: BASE_URL_PATH
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: s3-secret
                  key: awsKey
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: s3-secret
                  key: awsSecret
            - name: S3_ENDPOINT_URL
              valueFrom:
                secretKeyRef:
                  name: s3-secret
                  key: awsEndpointUrl                                    
            - name: DEFAULT_STORAGE
              valueFrom:
                configMapKeyRef:
                  name: api-server-configmap
                  key: DEFAULT_STORAGE
            - name: STORAGE_BINARY
              valueFrom:
                configMapKeyRef:
                  name: api-server-configmap
                  key: STORAGE_BINARY                  
            - name: CLUSTER_NAME
              valueFrom:
                configMapKeyRef:
                  name: api-server-configmap
                  key: CLUSTER_NAME
            - name: PIPELINE_DRIVER_CPU
              valueFrom:
                configMapKeyRef:
                  name: api-server-configmap
                  key: PIPELINE_DRIVER_CPU
            - name: PIPELINE_DRIVER_MEM
              valueFrom:
                configMapKeyRef:
                  name: api-server-configmap
                  key: PIPELINE_DRIVER_MEM
            - name: JAEGER_AGENT_SERVICE_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
---
# Source: hkube/templates/caching-service-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: caching-service
  labels:
    app: caching-service
    release: hkube
    heritage: Helm
    group: hkube
    core: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: caching-service
      release: hkube
  template:
    metadata:
      annotations:
        checksum/config: 6626b56cdc8e271efcdccb7ece4fb86e5b9d0164a6bfa95dc126c427ddb704fb
      labels:
        app: caching-service
        release: hkube
        group: hkube
    spec:        
      containers:
        - name: caching-service
          image: "hkube/caching-service:v1.3.1"
          imagePullPolicy: IfNotPresent
          resources:
            requests:
              cpu: 100m
              memory: 512Mi                
          ports:
            - containerPort: 9005
              protocol: TCP
          env:
            - name: NODE_ENV
              valueFrom:
                configMapKeyRef:
                  name: caching-service-configmap
                  key: NODE_ENV
            - name: CLUSTER_NAME
              valueFrom:
                configMapKeyRef:
                  name: caching-service-configmap
                  key: CLUSTER_NAME
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: s3-secret
                  key: awsKey
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: s3-secret
                  key: awsSecret
            - name: S3_ENDPOINT_URL
              valueFrom:
                secretKeyRef:
                  name: s3-secret
                  key: awsEndpointUrl                         
            - name: DEFAULT_STORAGE
              valueFrom:
                configMapKeyRef:
                  name: caching-service-configmap
                  key: DEFAULT_STORAGE
            - name: STORAGE_BINARY
              valueFrom:
                configMapKeyRef:
                  name: caching-service-configmap
                  key: STORAGE_BINARY                  
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
---
# Source: hkube/templates/job-cleaner-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: clean-old-jobs
  labels:
    app: clean-old-jobs
    release: hkube
    heritage: Helm
    group: hkube
    core: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: clean-old-jobs
      release: clean-old-jobs
  template:
    metadata:
      annotations:
        checksum/config: 5c97c6f4e84226f88a3ec3b9a7b51f05ba56da1821fcb229a92775416a44260e
      labels:
        app: clean-old-jobs
        release: clean-old-jobs
        group: hkube
    spec:
      serviceAccountName: clean-old-jobs
      containers:
        - name: clean-old-jobs
          image: "hkube/clean-old-jobs:v1.3.1"
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              path: /healthz
              port: 5000
            initialDelaySeconds: 100
            periodSeconds: 60
            timeoutSeconds: 1
            failureThreshold: 3          
          resources:
            requests:
              cpu: 100m
              memory: 512Mi 
          env:
            - name: NODE_ENV
              valueFrom:
                configMapKeyRef:
                  name: clean-old-jobs-configmap
                  key: NODE_ENV
            - name: "MAX_COMPLETED_JOB_AGE_HOURS"
              valueFrom:
                configMapKeyRef:
                  name: clean-old-jobs-configmap
                  key: MAX_COMPLETED_JOB_AGE_HOURS
            - name: "MAX_FAILED_JOB_AGE_HOURS"
              valueFrom:
                configMapKeyRef:
                  name: clean-old-jobs-configmap
                  key: MAX_FAILED_JOB_AGE_HOURS
            - name: "MAX_PENDING_JOB_AGE_HOURS"
              valueFrom:
                configMapKeyRef:
                  name: clean-old-jobs-configmap
                  key: MAX_PENDING_JOB_AGE_HOURS
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
---
# Source: hkube/templates/monitor-server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: monitor-server
  labels:
    app: monitor-server
    group: hkube
    core: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: monitor-server
  template:
    metadata:
      annotations:
        checksum/config: 1d214d41befc6d91696df255d144f49e0578b52dd4e847f5cb6f59e0b4f73bf1
      labels:
        app: monitor-server
        group: hkube
    spec: 
      serviceAccountName: monitor-server-serviceaccount
      containers:
        - name: monitor-server
          image: "hkube/monitor-server:v1.3.3"
          livenessProbe:
            httpGet:
              path: /healthz
              port: 5000
            initialDelaySeconds: 500
            periodSeconds: 60
            timeoutSeconds: 1
            failureThreshold: 3          
          resources:
            requests:
              cpu: 100m
              memory: 256Mi 
          env:
            - name: NODE_ENV
              valueFrom:
                configMapKeyRef:
                  name: monitor-server-configmap
                  key: NODE_ENV
            - name: CLUSTER_NAME
              valueFrom:
                configMapKeyRef:
                  name: monitor-server-configmap
                  key: CLUSTER_NAME
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: s3-secret
                  key: awsKey
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: s3-secret
                  key: awsSecret
            - name: S3_ENDPOINT_URL
              valueFrom:
                secretKeyRef:
                  name: s3-secret
                  key: awsEndpointUrl                                    
            - name: DEFAULT_STORAGE
              valueFrom:
                configMapKeyRef:
                  name: monitor-server-configmap
                  key: DEFAULT_STORAGE
            - name: STORAGE_BINARY
              valueFrom:
                configMapKeyRef:
                  name: monitor-server-configmap
                  key: STORAGE_BINARY                  
            - name: LOG_EXTERNAL_REQUESTS
              value: "false"
            - name: ELASTICSEARCH_SERVICE_URL
              value: ""
            - name: LOGS_VIEW_SOURCE
              value: "k8s"
          ports:
            - containerPort: 30010
          volumeMounts:
            - name: uploads
              mountPath: /hkube/monitor-server/uploads
            - name: versions-config
              mountPath: /versions  
      volumes:
        - name: versions-config
          configMap:
            name: hkube-versions
        - emptyDir: {}
          name: uploads
---
# Source: hkube/templates/pipeline-driver-queue-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pipeline-driver-queue
  labels:
    app: pipeline-driver-queue
    group: hkube
    core: "true"
    metrics-group: pipeline-driver-queue
spec:
  replicas: 
  selector:
    matchLabels:
      app: pipeline-driver-queue
  template:
    metadata:
      annotations:
        checksum/config: f7b075cfeff8f5f0a96421baca974590c36155f02650cf32739896bbdfd3c900
      labels:
        app: pipeline-driver-queue
        group: hkube
    spec:
      containers:
        - name: pipeline-driver-queue
          image: "hkube/pipeline-driver-queue:v1.3.1"
          imagePullPolicy: IfNotPresent
          resources:
            requests:
              cpu: 100m
              memory: 512Mi 
          ports:
            - name: http
              containerPort: 3000
              protocol: TCP
          env:
            - name: NODE_ENV
              valueFrom:
                configMapKeyRef:
                  name: pipeline-driver-queue-configmap
                  key: NODE_ENV
            - name: HKUBE_LOG_LEVEL
              valueFrom:
                configMapKeyRef:
                  name: pipeline-driver-queue-configmap
                  key: KUBE_LOG_LEVEL
            - name: METRICS_PORT
              valueFrom:
                configMapKeyRef:
                  name: pipeline-driver-queue-configmap
                  key: METRICS_PORT
            - name: CHECK_QUEUE_INTERVAL
              valueFrom:
                configMapKeyRef:
                  name: pipeline-driver-queue-configmap
                  key: CHECK_QUEUE_INTERVAL
            - name: JAEGER_AGENT_SERVICE_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
---
# Source: hkube/templates/resource-manager-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resource-manager
  labels:
    app: resource-manager
    scale-group: resource-manager
    group: hkube
    core: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: resource-manager
  template:
    metadata:
      annotations:
        checksum/config: a85f6f1d91b0c91de60d6d3bacdea5679ffd533420424ba83c1c548f127a658f
      labels:
        app: resource-manager
        group: hkube
    spec:
      serviceAccountName: resource-manager-serviceaccount
      containers:
        - name: resource-manager
          image: "hkube/resource-manager:v1.3.1"          
          resources:
            requests:
              cpu: 100m
              memory: 512Mi 
          ports:
            - containerPort: 3000
          env:
            - name: NODE_ENV
              valueFrom:
                configMapKeyRef:
                  name: resource-manager-configmap
                  key: NODE_ENV
            - name: HKUBE_LOG_LEVEL
              valueFrom:
                configMapKeyRef:
                  name: resource-manager-configmap
                  key: HKUBE_LOG_LEVEL
            - name: INTERVAL
              value: "1000"
            - name: ALGORITHMS_THRESHOLD_CPU
              valueFrom:
                configMapKeyRef:
                  name: resource-manager-configmap
                  key: ALGORITHMS_THRESHOLD_CPU
            - name: ALGORITHMS_THRESHOLD_MEM
              valueFrom:
                configMapKeyRef:
                  name: resource-manager-configmap
                  key: ALGORITHMS_THRESHOLD_MEM
            - name: DRIVERS_THRESHOLD_CPU
              valueFrom:
                configMapKeyRef:
                  name: resource-manager-configmap
                  key: DRIVERS_THRESHOLD_CPU
            - name: DRIVERS_THRESHOLD_MEM
              valueFrom:
                configMapKeyRef:
                  name: resource-manager-configmap
                  key: DRIVERS_THRESHOLD_MEM
            - name: METRICS_PORT
              valueFrom:
                configMapKeyRef:
                  name: resource-manager-configmap
                  key: METRICS_PORT
            - name: PROMETHEUS_ENDPOINT
              valueFrom:
                configMapKeyRef:
                  name: resource-manager-configmap
                  key: PROMETHEUS_ENDPOINT
            - name: JAEGER_AGENT_SERVICE_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
---
# Source: hkube/templates/simulator-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: simulator
  labels:
    app: simulator
    group: hkube
    core: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: simulator
  template:
    metadata:
      annotations:
        checksum/config: 256d24ea6e327063fb7dc0949fe8c6674cb4ebec0f6cfa89dabcea2a6ff13e8f
      labels:
        app: simulator
        group: hkube
    spec:
      containers:
        - name: simulator
          image: "hkube/simulator:v1.3.3"
          resources:
            requests:
              cpu: 100m
              memory: 512Mi 
          ports:
            - containerPort: 9050
          env:
            - name: MONITOR_BACKEND_PATH
              valueFrom:
                configMapKeyRef:
                  name: simulator-configmap
                  key: MONITOR_BACKEND_PATH
            - name: MONITOR_BACKEND_PATH_SOCKETIO
              valueFrom:
                configMapKeyRef:
                  name: simulator-configmap
                  key: MONITOR_BACKEND_PATH_SOCKETIO
            - name: MONITOR_BACKEND_USE_LOCATION
              valueFrom:
                configMapKeyRef:
                  name: simulator-configmap
                  key: MONITOR_BACKEND_USE_LOCATION
            - name: MONITOR_BACKEND_HOST
              valueFrom:
                configMapKeyRef:
                  name: simulator-configmap
                  key: MONITOR_BACKEND_HOST
            - name: MONITOR_BACKEND_PORT
              valueFrom:
                configMapKeyRef:
                  name: simulator-configmap
                  key: MONITOR_BACKEND_PORT
            - name: isSecure
              valueFrom:
                configMapKeyRef:
                  name: simulator-configmap
                  key: MONITOR_BACKEND_IS_SECURE
            - name: BOARD_PATH
              valueFrom:
                configMapKeyRef:
                  name: simulator-configmap
                  key: BOARD_PATH
            - name: BOARD_USE_LOCATION
              valueFrom:
                configMapKeyRef:
                  name: simulator-configmap
                  key: BOARD_USE_LOCATION
            - name: BOARD_HOST
              valueFrom:
                configMapKeyRef:
                  name: simulator-configmap
                  key: BOARD_HOST
            - name: BOARD_PORT
              valueFrom:
                configMapKeyRef:
                  name: simulator-configmap
                  key: BOARD_PORT
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
---
# Source: hkube/templates/task-executor-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: task-executor
  labels:
    app: task-executor
    scale-group: task-executor
    group: hkube
    core: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: task-executor
  template:
    metadata:
      annotations:
        checksum/config: 355167592c9ed14cc391c4576d33801dd246f8c224ca79d0f34c8d972631d64b
      labels:
        app: task-executor
        group: hkube
    spec:
      serviceAccountName: task-executor-serviceaccount
      containers:
        - name: task-executor
          image: "hkube/task-executor:v1.3.3"
          livenessProbe:
            httpGet:
              path: /healthz
              port: 5000
            initialDelaySeconds: 100
            periodSeconds: 20
            timeoutSeconds: 1
            failureThreshold: 3
          resources:
            requests:
              cpu: 100m
              memory: 512Mi 
          ports:
            - containerPort: 3000
          env:
            - name: NODE_ENV
              valueFrom:
                configMapKeyRef:
                  name: task-executor-configmap
                  key: NODE_ENV
            - name: IS_PRIVILEGED
              valueFrom:
                configMapKeyRef:
                  name: task-executor-configmap
                  key: IS_PRIVILEGED
            - name: IS_NAMESPACED
              valueFrom:
                configMapKeyRef:
                  name: task-executor-configmap
                  key: IS_NAMESPACED
            - name: PIPELINE_DRIVERS_AMOUNT
              valueFrom:
                configMapKeyRef:
                  name: task-executor-configmap
                  key: PIPELINE_DRIVERS_AMOUNT
            - name: METRICS_PORT
              valueFrom:
                configMapKeyRef:
                  name: task-executor-configmap
                  key: METRICS_PORT
            - name: JAEGER_AGENT_SERVICE_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: DEFAULT_STORAGE
              valueFrom:
                configMapKeyRef:
                  name: task-executor-configmap
                  key: DEFAULT_STORAGE
            - name: STORAGE_BINARY
              valueFrom:
                configMapKeyRef:
                  name: task-executor-configmap
                  key: STORAGE_BINARY                  
            - name: BASE_FS_ADAPTER_DIRECTORY
              valueFrom:
                configMapKeyRef:
                  name: task-executor-configmap
                  key: BASE_FS_ADAPTER_DIRECTORY
            - name: WORKER_SOCKET_MAX_PAYLOAD_BYTES
              valueFrom:
                configMapKeyRef:
                  name: task-executor-configmap
                  key: WORKER_SOCKET_MAX_PAYLOAD_BYTES
            - name: HAS_NODE_LIST
              valueFrom:
                configMapKeyRef:
                  name: task-executor-configmap
                  key: HAS_NODE_LIST
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: WORKER_MEMORY
              value: "512"
            - name: WORKER_CPU
              value: "0.1"
            - name: RESOURCES_ENABLE
              value: "false"        
            - name: INTERVAL_MS
              value: "1000"
            - name: LOG_EXTERNAL_REQUESTS
              value: "false"
            - name: DEFAULT_QUOTA_CPU
              value: "30"
            - name: DEFAULT_QUOTA_MEM
              value: "20Gi"
---
# Source: hkube/templates/trigger-service-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: trigger-service
  labels:
    app: trigger-service
    scale-group: trigger-service
    group: hkube
    core: "true"
    metrics-group: trigger-service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: trigger-service
  template:
    metadata:
      annotations:
        checksum/config: d76bf3203946bdb86d7ede8753875409ec52cefd78c5fcdaad3d7d0231e23cdf
      labels:
        app: trigger-service
        group: hkube
    spec:
      containers:
        - name: trigger-service
          image: "hkube/trigger-service:v1.3.1"
          resources:
            requests:
              cpu: 100m
              memory: 256Mi          
          ports:
            - containerPort: 3000
          env:
            - name: NODE_ENV
              valueFrom:
                configMapKeyRef:
                  name: trigger-service-configmap
                  key: NODE_ENV
            - name: METRICS_PORT
              valueFrom:
                configMapKeyRef:
                  name: trigger-service-configmap
                  key: METRICS_PORT
            - name: JAEGER_AGENT_SERVICE_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
---
# Source: hkube/charts/etcd/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: hkube-etcd
  labels:
    heritage: "Helm"
    release: "hkube"
    chart: "etcd-0.7.2007"
    app: etcd
    app.kubernetes.io/name: etcd
    app.kubernetes.io/instance: hkube
spec:
  selector:
    matchLabels:
      app: etcd
      #app.kubernetes.io/instance: hkube
  serviceName: hkube-etcd
  replicas: 3
  template:
    metadata:
      name: hkube-etcd
      labels:
        heritage: "Helm"
        release: "hkube"
        chart: "etcd-0.7.2007"
        app: etcd
        affinity: etcd
    spec:
      containers:
      - name: hkube-etcd
        image: "hkube/etcd:3.3.1"
        imagePullPolicy: "IfNotPresent"
        ports:
        - containerPort: 2380
          name: peer
        - containerPort: 2379
          name: client
        resources:
          {}          
        env:
        - name: ETCD_HEARTBEAT_INTERVAL
          value: "1000"
        - name: ETCD_ELECTION_TIMEOUT
          value: "5000"
        - name: ETCD_AUTO_COMPACTION_RETENTION
          value: "5m"
        - name: ETCD_AUTO_COMPACTION_MODE
          value: "periodic"
        - name: ETCD_QUOTA_BACKEND_BYTES
        # 8GB
          value: "8589934592"
        - name: ETCDCTL_API
          value: "3"
        - name: INITIAL_CLUSTER_SIZE
          value: "3"
        - name: SET_NAME
          value: hkube-etcd
        volumeMounts:
        - name: datadir
          mountPath: /var/run/etcd             
        lifecycle:
          preStop:
            exec:
              command:
                - "/bin/sh"
                - "-ec"
                - |
                  EPS=""
                  for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                      EPS="${EPS}${EPS:+,}http://${SET_NAME}-${i}.${SET_NAME}:2379"
                  done

                  HOSTNAME=$(hostname)
                  AUTH_OPTIONS=""                  

                  member_hash() {
                      etcdctl $AUTH_OPTIONS member list | grep http://${HOSTNAME}.${SET_NAME}:2380 | cut -d ',' -f1
                  }

                  SET_ID=${HOSTNAME##*[^0-9]}

                  if [ "${SET_ID}" -ge ${INITIAL_CLUSTER_SIZE} ]; then
                      echo "Removing ${HOSTNAME} from etcd cluster"
                      ETCDCTL_ENDPOINT=${EPS} etcdctl $AUTH_OPTIONS member remove $(member_hash)
                      if [ $? -eq 0 ]; then
                          # Remove everything otherwise the cluster will no longer scale-up
                          rm -rf /var/run/etcd/*
                      fi
                  fi
        command:
          - "/bin/sh"
          - "-ec"
          - |
            HOSTNAME=$(hostname)
            AUTH_OPTIONS=""
            # store member id into PVC for later member replacement
            collect_member() {                
                while ! etcdctl $AUTH_OPTIONS member list > /dev/null 2>&1; do sleep 1; done
                etcdctl $AUTH_OPTIONS member list | grep http://${HOSTNAME}.${SET_NAME}:2380 | cut -d ',' -f1 > /var/run/etcd/member_id
                exit 0
            }

            eps() {
                EPS=""
                for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                    EPS="${EPS}${EPS:+,}http://${SET_NAME}-${i}.${SET_NAME}:2379"
                done
                echo ${EPS}
            }

            member_hash() {
                etcdctl $AUTH_OPTIONS member list | grep http://${HOSTNAME}.${SET_NAME}:2380 | cut -d ',' -f1
            }

            # we should wait for other pods to be up before trying to join
            # otherwise we got "no such host" errors when trying to resolve other members
            for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                while true; do
                    echo "Waiting for ${SET_NAME}-${i}.${SET_NAME} to come up"
                    ping -W 1 -c 1 ${SET_NAME}-${i}.${SET_NAME} > /dev/null && break
                    sleep 1s
                done                
            done
            
            # re-joining after failure?
            if [ -e /var/run/etcd/default.etcd && -e /var/run/etcd/member_id ]; then
                echo "Re-joining etcd member"
                member_id=$(cat /var/run/etcd/member_id)

                # re-join member
                ETCDCTL_ENDPOINT=$(eps) etcdctl $AUTH_OPTIONS member update ${member_id} http://${HOSTNAME}.${SET_NAME}:2380 | true
                exec etcd --name ${HOSTNAME} \
                    --listen-peer-urls http://0.0.0.0:2380 \
                    --listen-client-urls http://0.0.0.0:2379\
                    --advertise-client-urls http://${HOSTNAME}.${SET_NAME}:2379 \
                    --data-dir /var/run/etcd/default.etcd
                    
            fi

            # etcd-SET_ID
            SET_ID=${HOSTNAME##*[^0-9]}

            # adding a new member to existing cluster (assuming all initial pods are available)
            if [ "${SET_ID}" -ge ${INITIAL_CLUSTER_SIZE} ]; then
                export ETCDCTL_ENDPOINT=$(eps)

                # member already added?
                MEMBER_HASH=$(member_hash)
                if [ -n "${MEMBER_HASH}" ]; then
                    # the member hash exists but for some reason etcd failed
                    # as the datadir has not be created, we can remove the member
                    # and retrieve new hash
                    etcdctl $AUTH_OPTIONS member remove ${MEMBER_HASH}
                fi

                echo "Adding new member"
                etcdctl $AUTH_OPTIONS member add ${HOSTNAME} http://${HOSTNAME}.${SET_NAME}:2380 | grep "^ETCD_" > /var/run/etcd/new_member_envs

                if [ $? -ne 0 ]; then
                    echo "Exiting"
                    rm -f /var/run/etcd/new_member_envs
                    exit 1
                fi

                cat /var/run/etcd/new_member_envs
                source /var/run/etcd/new_member_envs

                collect_member &

                exec etcd --name ${HOSTNAME} \
                    --listen-peer-urls http://0.0.0.0:2380 \
                    --listen-client-urls http://0.0.0.0:2379 \
                    --advertise-client-urls http://${HOSTNAME}.${SET_NAME}:2379 \
                    --data-dir /var/run/etcd/default.etcd \
                    --initial-advertise-peer-urls http://${HOSTNAME}.${SET_NAME}:2380 \
                    --initial-cluster ${ETCD_INITIAL_CLUSTER} \
                    --initial-cluster-state ${ETCD_INITIAL_CLUSTER_STATE}
                    
            fi

            PEERS=""
            for i in $(seq 0 $((${INITIAL_CLUSTER_SIZE} - 1))); do
                PEERS="${PEERS}${PEERS:+,}${SET_NAME}-${i}=http://${SET_NAME}-${i}.${SET_NAME}:2380"
            done

            collect_member &

            # join member
            exec etcd --name ${HOSTNAME} \
                --initial-advertise-peer-urls http://${HOSTNAME}.${SET_NAME}:2380 \
                --listen-peer-urls http://0.0.0.0:2380 \
                --listen-client-urls http://0.0.0.0:2379 \
                --advertise-client-urls http://${HOSTNAME}.${SET_NAME}:2379 \
                --initial-cluster-token etcd-cluster-1 \
                --initial-cluster ${PEERS} \
                --initial-cluster-state new \
                --data-dir /var/run/etcd/default.etcd
  volumeClaimTemplates:
  - metadata:
      name: datadir
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          # upstream recommended max is 700M
          storage: "1Gi"
---
# Source: hkube/charts/jaeger/charts/cassandra/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: hkube-cassandra
  labels:
    app: cassandra
    chart: cassandra-0.15.0
    release: hkube
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: cassandra
      release: hkube
  serviceName: hkube-cassandra
  replicas: 1
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: OnDelete
  template:
    metadata:
      labels:
        app: cassandra
        release: hkube
    spec:
      hostNetwork: false
      initContainers:
      - name: config-copier
        image: cassandra:3.11.6
        command: [ 'sh', '-c', 'cp -r /etc/cassandra/* /cassandra-configs/']
        volumeMounts:
        - name: cassandra-configs
          mountPath: /cassandra-configs/
      containers:
      - name: hkube-cassandra
        image: "cassandra:3.11.6"
        imagePullPolicy: "IfNotPresent"
        command: [ '/bin/sh', '-c', 'docker-entrypoint.sh cassandra -R -f']
#
        resources:
          {}
        env:
        - name: CASSANDRA_SEEDS
          value: "hkube-cassandra-0.hkube-cassandra.default.svc.cluster.local"
        - name: MAX_HEAP_SIZE
          value: "2048M"
        - name: HEAP_NEWSIZE
          value: "512M"
        - name: CASSANDRA_ENDPOINT_SNITCH
          value: "GossipingPropertyFileSnitch"
        - name: CASSANDRA_CLUSTER_NAME
          value: "jaeger"
        - name: CASSANDRA_DC
          value: "dc1"
        - name: CASSANDRA_RACK
          value: "rack1"
        - name: CASSANDRA_START_RPC
          value: "false"
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        livenessProbe:
          exec:
            command: [ "/bin/sh", "-c", "nodetool status" ]
          initialDelaySeconds: 90
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
        readinessProbe:
          exec:
            command: [ "/bin/sh", "-c", "nodetool status | grep -E \"^UN\\s+${POD_IP}\"" ]
          initialDelaySeconds: 90
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
        ports:
        - name: intra
          containerPort: 7000
        - name: tls
          containerPort: 7001
        - name: jmx
          containerPort: 7199
        - name: cql
          containerPort: 9042
        - name: thrift
          containerPort: 9160
        volumeMounts:
        - name: data
          mountPath: /var/lib/cassandra
        - name: cassandra-configs
          mountPath: /etc/cassandra
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "exec nodetool decommission"]
      terminationGracePeriodSeconds: 30
      volumes:
      - name: cassandra-configs
        emptyDir: {}
      - name: data
        emptyDir: {}
---
# Source: hkube/charts/redis-ha/templates/redis-ha-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: hkube-redis-ha-server
  labels:
    app: redis-ha
    heritage: "Helm"
    release: "hkube"
    chart: redis-ha-3.6.3
spec:
  selector:
    matchLabels:
      release: hkube
      app: redis-ha
  serviceName: hkube-redis-ha
  replicas: 3
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/init-config: 4ee46b4f4c3037097b498a828132a8aef6b1b618a4873131610a37df6a5b19f6
        checksum/probe-config: 3f199787b06cd580455d0606a2077ad5e0c8b1e706cc2df71a52a6802540e1fd
      labels:
        release: hkube
        app: redis-ha
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: redis-ha
                  release: hkube
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app:  redis-ha
                    release: hkube
                topologyKey: failure-domain.beta.kubernetes.io/zone
      securityContext:
        fsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
      serviceAccountName: hkube-redis-ha
      initContainers:
      - name: config-init
        image: redis:5.0.5-alpine
        imagePullPolicy: IfNotPresent
        resources:
          {}
        command:
        - sh
        args:
        - /readonly-config/init.sh
        env:
        - name: SENTINEL_ID_0
          value: dd70b98f75d485659d89b2a8de4621324260c42d

        - name: SENTINEL_ID_1
          value: c30b452b1ded087c96738bc868714bbd0739908b

        - name: SENTINEL_ID_2
          value: 6a7fbda6e93b80cec7f86e6545b634d610a381fe

        volumeMounts:
        - name: config
          mountPath: /readonly-config
          readOnly: true
        - name: data
          mountPath: /data
      containers:
      - name: redis
        image: redis:5.0.5-alpine
        imagePullPolicy: IfNotPresent
        command:
        - redis-server
        args:
        - /data/conf/redis.conf
        livenessProbe:
          exec:
            command: [ "sh", "/probes/liveness.sh", "6379"]
          initialDelaySeconds: 15
          periodSeconds: 5
        readinessProbe:
          exec:
            command: ["sh", "/probes/readiness.sh", "6379"]
          initialDelaySeconds: 15
          periodSeconds: 5
        resources:
          {}
        ports:
        - name: redis
          containerPort: 6379
        volumeMounts:
        - mountPath: /data
          name: data
        - mountPath: /probes
          name: probes
      - name: sentinel
        image: redis:5.0.5-alpine
        imagePullPolicy: IfNotPresent
        command:
          - redis-sentinel
        args:
          - /data/conf/sentinel.conf
        livenessProbe:
          exec:
            command: [ "sh", "/probes/readiness.sh", "26379"]
          initialDelaySeconds: 15
          periodSeconds: 5
        readinessProbe:
          exec:
            command: ["sh", "/probes/readiness.sh", "26379"]
          initialDelaySeconds: 15
          periodSeconds: 5
        resources:
          {}
        ports:
          - name: sentinel
            containerPort: 26379
        volumeMounts:
        - mountPath: /data
          name: data
        - mountPath: /probes
          name: probes
      volumes:
      - name: config
        configMap:
          name: hkube-redis-ha-configmap
      - name: probes
        configMap:
          name: hkube-redis-ha-probes
      - name: data
        emptyDir: {}
---
# Source: hkube/charts/jaeger/templates/cassandra-schema-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: jaeger-cassandra-schema
  labels:
    helm.sh/chart: jaeger-0.27.2004
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: hkube
    app.kubernetes.io/version: "1.17.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: cassandra-schema
spec:
  activeDeadlineSeconds: 300
  template:
    metadata:
      name: jaeger-cassandra-schema
    spec:
      serviceAccountName: jaeger-cassandra-schema
      containers:
      - name: jaeger-cassandra-schema
        image: jaegertracing/jaeger-cassandra-schema:1.17.1
        imagePullPolicy: IfNotPresent
        env:
        
        - name: "MODE"
          value: "prod"
        
        - name: CASSANDRA_SERVERS
          value: hkube-cassandra
        - name: CASSANDRA_PORT
          value: "9042"
        
        - name: CASSANDRA_KEYSPACE
          value: jaeger_v1_test
        - name: CASSANDRA_USERNAME
          value: user
        - name: CASSANDRA_PASSWORD
          valueFrom:
            secretKeyRef:
              name: jaeger-cassandra
              key: password
        - name: CQLSH_HOST
          value: hkube-cassandra
        
        - name: DATACENTER
          value: "dc1"
        - name: KEYSPACE
          value: jaeger_v1_test
        resources:
          {}
        volumeMounts:
      restartPolicy: OnFailure
      volumes:
---
# Source: hkube/templates/etcd-cleaner-deployment.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: etcd-cleaner
  labels:
    group: hkube
    core: "true"
    app: "etcd-cleaner"
spec:
  schedule: "0 0 * * *" 
  startingDeadlineSeconds: 200
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: etcd-cleaner
            image: "hkube/etcd-cleaner:v1.3.4"
            resources:
              requests:
                cpu: 100m
                memory: 512Mi 
            env:
            - name: "OBJECT_EXPIRATION_DAYS"
              valueFrom:
                configMapKeyRef:
                  name: etcd-cleaner-configmap
                  key: OBJECT_EXPIRATION_DAYS
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          restartPolicy: Never
---
# Source: hkube/templates/etcd-defrag.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: etcd-defrag-cron
  labels:
    release: hkube
    heritage: Helm
    group: hkube
    core: "true"
spec:
  schedule: "*/10 * * * *" 
  startingDeadlineSeconds: 200
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: clean-old-jobs
          containers:
          - name: etcd-defrag-cron
            image: hkube/etcd-defrag-cron:v1.0.0
            resources:
              requests:
                cpu: 100m
                memory: 512Mi 
          restartPolicy: Never
---
# Source: hkube/templates/pipeline-cleaner.deployment.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: pipeline-cleaner
  labels:
    group: hkube
    core: "true"
    app: "pipeline-cleaner"
spec:
  schedule: "*/5 * * * *" 
  startingDeadlineSeconds: 200
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: "pipeline-cleaner"
        spec:
          containers:
          - name: pipeline-cleaner
            image: "hkube/pipeline-cleaner:v1.3.1"
            resources:
              requests:
                cpu: 100m
                memory: 512Mi 
            env:
              - name: NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
          restartPolicy: Never
---
# Source: hkube/templates/storage-cleaner-deployment.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: storage-cleaner
  labels:
    group: hkube
    core: "true"
    app: storage-cleaner
spec:

  schedule: "0 0 * * *" 
  startingDeadlineSeconds: 200
  jobTemplate:
    spec:
      template:
        spec:          
          containers:
          - name: storage-cleaner
            image: "hkube/storage-cleaner:v1.3.2"
            resources:
              requests:
                cpu: 100m
                memory: 512Mi 
            env:
            - name: "TEMP_OBJECT_EXPIRATION_DAYS"
              valueFrom:
                configMapKeyRef:
                  name: storage-cleaner-configmap
                  key: TEMP_OBJECT_EXPIRATION_DAYS
            - name: "RESULT_OBJECT_EXPIRATION_DAYS"
              valueFrom:
                configMapKeyRef:
                  name: storage-cleaner-configmap
                  key: RESULT_OBJECT_EXPIRATION_DAYS
            - name: CLUSTER_NAME
              valueFrom:
                configMapKeyRef:
                  name: storage-cleaner-configmap
                  key: CLUSTER_NAME
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: s3-secret
                  key: awsKey
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: s3-secret
                  key: awsSecret
            - name: S3_ENDPOINT_URL
              valueFrom:
                secretKeyRef:
                  name: s3-secret
                  key: awsEndpointUrl  
            - name: DEFAULT_STORAGE
              valueFrom:
                configMapKeyRef:
                  name: storage-cleaner-configmap
                  key: DEFAULT_STORAGE
          restartPolicy: Never
---
# Source: hkube/charts/etcd/templates/etcd-ui.yml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: etcd-ui
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  labels:
    app: etcd-ui
    release: hkube
    heritage: Helm
    core: "true"
spec:
  rules:
    - http:
        paths:
        - path: /hkube/etcd-ui(/|$)(.*)
          backend:
            serviceName: etcd-ui
            servicePort: 8080
---
# Source: hkube/charts/jaeger/templates/query-ing.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: jaeger-query
  labels:
    helm.sh/chart: jaeger-0.27.2004
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: hkube
    app.kubernetes.io/version: "1.17.1"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: query
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: "500m"
spec:
  rules:
    - http:
        paths:
        - path: /jaeger(/|$)(.*)
          backend:
            serviceName: jaeger-query
            servicePort: 80
---
# Source: hkube/templates/api-server-ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: api-server
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: "500m"    
  labels:
    app: api-server
    release: hkube
    heritage: Helm
    group: hkube
    core: "true"
spec:
  rules:
    - http:
        paths:
        - path: /hkube/api-server(/|$)(.*)
          backend:
            serviceName: api-server
            servicePort: 3000
---
# Source: hkube/templates/monitor-server-ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: monitor-server
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: "500m"    
  labels:
    app: monitor-server
    release: hkube
    heritage: Helm
    group: hkube
    core: "true"
spec:
  rules:
    - http:
        paths:
        - path: /hkube/monitor-server(/|$)(.*)        
          backend:
            serviceName: monitor-server
            servicePort: 30010
---
# Source: hkube/templates/simulator-ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: dashboard
  annotations:
    nginx.ingress.kubernetes.io/configuration-snippet: |
      if ( $request_filename ~ /hkube/simulator ){
        rewrite ^ /hkube/dashboard/ permanent;
      }
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
  labels:
    app: simulator
    release: hkube
    heritage: Helm
    group: hkube
    core: "true"
spec:
  rules:
    - http:
        paths:
        - path: /hkube/dashboard(/|$)(.*)
          backend:
            serviceName: simulator
            servicePort: 9050
        - path: /hkube/simulator(/|$)(.*)
          backend:
            serviceName: simulator
            servicePort: 9050
---
# Source: hkube/charts/redis-ha/templates/tests/test-redis-ha-configmap.yaml
apiVersion: v1
kind: Pod
metadata:
  name: hkube-redis-ha-configmap-test
  labels:
    app: redis-ha
    heritage: "Helm"
    release: "hkube"
    chart: redis-ha-3.6.3
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
  - name: check-init
    image: koalaman/shellcheck:v0.5.0
    args:
    - --shell=sh
    - /readonly-config/init.sh
    volumeMounts:
    - name: config
      mountPath: /readonly-config
      readOnly: true
  - name: check-probes
    image: koalaman/shellcheck:v0.5.0
    args:
    - --shell=sh
    - /probes/check-quorum.sh
    volumeMounts:
    - name: probes
      mountPath: /probes
      readOnly: true
  volumes:
  - name: config
    configMap:
      name: hkube-redis-ha-configmap
  - name: probes
    configMap:
      name: hkube-redis-ha-probes
  restartPolicy: Never
---
# Source: hkube/charts/redis-ha/templates/tests/test-redis-ha-service.yaml
apiVersion: v1
kind: Pod
metadata:
  name: hkube-redis-ha-service-test
  labels:
    app: redis-ha
    heritage: "Helm"
    release: "hkube"
    chart: redis-ha-3.6.3
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
  - name: "hkube-service-test"
    image: redis:5.0.5-alpine
    command:
      - sh
      - -c
      - redis-cli -h hkube-redis-ha -p 6379 info server
  restartPolicy: Never
---
# Source: hkube/templates/pre-delete.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: "pre-del-dep-and-jobs"
  labels:
    chart: "hkube-0.1.2"
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    "resource-policy": delete-on-completion
    "helm.sh/hook": pre-delete
    "helm.sh/hook-weight": "-5"
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  template:
    metadata:
      name: "post-del-dep-and-jobs"
      labels:
        heritage: "Helm"
        release: "hkube"
        chart: "hkube-0.1.2"
    spec:
      serviceAccountName: cleanup-serviceaccount
      restartPolicy: Never
      containers:
      - name: post-del-dep-and-jobs
        image: "hkube/del-dep-and-jobs:v1.0.8"
        resources:
          requests:
            cpu: 100m
            memory: 512Mi 
        command:
          - /bin/sh
          - -c
          - |
            /usr/local/bin/kubectl cluster-info
            /usr/local/bin/kubectl delete deploy task-executor
            /usr/local/bin/kubectl delete deploy algorithm-operator
            /usr/local/bin/kubectl delete deploy -l type=algorithm-queue
            /usr/local/bin/kubectl delete job -l type=worker
            /usr/local/bin/kubectl delete job -l type=pipeline-driver
            /usr/local/bin/kubectl delete deploy -l type=worker-debug
            /usr/local/bin/kubectl delete svc -l type=worker-debug
            /usr/local/bin/kubectl delete secret s3-secret
            /usr/local/bin/kubectl delete crd etcdclusters.etcd.database.coreos.com
            echo Done
---
# Source: hkube/templates/pre-upgrade.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: "post-upgrade-dep-and-jobs"
  labels:
    chart: "hkube-0.1.2"
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    "resource-policy": delete-on-completion
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-weight": "-5"
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  template:
    metadata:
      name: "post-del-dep-and-jobs"
      labels:
        heritage: "Helm"
        release: "hkube"
        chart: "hkube-0.1.2"
    spec:
      serviceAccountName: cleanup-serviceaccount
      restartPolicy: Never
      containers:
      - name: post-del-dep-and-jobs
        image: "hkube/del-dep-and-jobs:v1.0.8"
        resources:
          requests:
            cpu: 100m
            memory: 512Mi 

        command:
          - /bin/sh
          - -c
          - |
            /usr/local/bin/kubectl delete deploy -l type=algorithm-queue
            /usr/local/bin/kubectl delete job -l type=worker
            /usr/local/bin/kubectl delete job -l type=pipeline-driver
            /usr/local/bin/kubectl delete deploy -l type=worker-debug
            /usr/local/bin/kubectl delete svc -l type=worker-debug
            /usr/local/bin/kubectl delete ingress -l type=worker-debug
            echo Done
